{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = pd.read_csv('new_patient_notes.csv')  \n",
    "y1 = pd.read_csv('new_primary_diagnosis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pairs(pred_pairs, ground_truth_clusters):\n",
    "    true_pairs = set()\n",
    "    for cluster in ground_truth_clusters:\n",
    "        for a in range(len(cluster)):\n",
    "            for b in range(a+1, len(cluster)):\n",
    "                i,j = cluster[a], cluster[b]\n",
    "                true_pairs.add((min(i,j), max(i,j)))\n",
    "\n",
    "    all_pairs = pred_pairs.union(true_pairs)\n",
    "    y_true = [1 if p in true_pairs else 0 for p in all_pairs]\n",
    "    y_pred = [1 if p in pred_pairs  else 0 for p in all_pairs]\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred,\n",
    "        average='binary',\n",
    "        zero_division=0\n",
    "    )\n",
    "    return {'precision': p, 'recall': r, 'f_score': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/var/folders/lj/5mq9mzlx1t58q2x7hr_p8l5c0000gn/T/ipykernel_40782/1864534233.py:20: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  'windows', 'win(?:\\d+)?', 'pro', 'home', 'ultimate',\n"
     ]
    }
   ],
   "source": [
    "class EntityResolutionPipeline:\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.cleaned_data = None\n",
    "        self.blocks = defaultdict(list)\n",
    "        self.candidate_pairs = set()\n",
    "        # self.matches = set()\n",
    "        # self.clusters = []\n",
    "        \n",
    "    def set_data(self, data):\n",
    "        self.data = data.copy()\n",
    "        self.cleaned_data = data.copy()\n",
    "        self.cleaned_data['cleaned_title'] = self.cleaned_data['notes']\n",
    "        return self.data\n",
    "    \n",
    "    def clean_data(self):\n",
    "        self.cleaned_data = self.data.copy()\n",
    "        extra_words = {\n",
    "            'bundle', 'warranty', 'deal', 'offers?', 'shipping',\n",
    "            'windows', 'win(?:\\d+)?', 'pro', 'home', 'ultimate',\n",
    "            'beats', 'audio'\n",
    "        } \n",
    "        common_words = [\n",
    "            'vology', '-', '/', '\"', 'hdd', 'ssd', 'windows ', 'windows',\n",
    "            'laptop', 'new', 'sale', 'core', 'buy', 'computers', 'computer', \n",
    "            'computers', 'com'\n",
    "        ]\n",
    "        def clean_title(title):\n",
    "            title = title.lower()\n",
    "            title = re.sub(r'[!@#$%^&*()_+={}\\[\\]|\\\\:;<>?,./\"]', ' ', title)\n",
    "            title_tokens = title.split()\n",
    "            title_tokens = [token for token in title_tokens if token not in ENGLISH_STOP_WORDS]\n",
    "            title = ' '.join(title_tokens)\n",
    "            title = re.sub(r'(\\d+)\\s*gb', r'\\1gb', title)  \n",
    "            title = re.sub(r'(\\d+)\\s*tb', r'\\1tb', title) \n",
    "            title = re.sub(r'core\\s*i(\\d)', r'corei\\1', title)  \n",
    "            title = re.sub(r'windows\\s*(\\d+)', r'windows\\1', title) \n",
    "            title = re.sub(r'gigs', 'gb', title)\n",
    "            \n",
    "            for i in common_words:\n",
    "                title = title.replace(i, '')\n",
    "\n",
    "            for i in extra_words:\n",
    "                title = title.replace(i, '')\n",
    "            \n",
    "            return title\n",
    "        \n",
    "        self.cleaned_data['cleaned_title'] = self.cleaned_data['notes'].apply(clean_title)\n",
    "        return self.cleaned_data\n",
    "    \n",
    "    def create_blocks(self):    \n",
    "        self.blocks = defaultdict(list)\n",
    "        \n",
    "        for idx, row in self.cleaned_data.iterrows():\n",
    "            tokens = row['cleaned_title'].split()\n",
    "            for token in tokens:\n",
    "                if len(token) > 2:  \n",
    "                    self.blocks[token].append(row['patient_id'])\n",
    "        \n",
    "        return self.blocks\n",
    "    \n",
    "    def filter_blocks(self, tau):\n",
    "        filtered_blocks = {token: records for token, records in self.blocks.items() \n",
    "                          if len(records) < tau and len(records) > 1}       \n",
    "        self.candidate_pairs = set()      \n",
    "        for records in filtered_blocks.values():\n",
    "            for i in range(len(records)):\n",
    "                for j in range(i+1, len(records)):\n",
    "                    self.candidate_pairs.add((records[i], records[j]))   \n",
    "        return self.candidate_pairs\n",
    "    \n",
    "    def compute_jaccard_similarity(self, id1, id2):\n",
    "        if not hasattr(self, '_id_to_row_index'):\n",
    "            self._id_to_row_index = {row['patient_id']: i for i, row in self.cleaned_data.iterrows()}\n",
    "        try:\n",
    "            idx1 = self._id_to_row_index[id1]\n",
    "            idx2 = self._id_to_row_index[id2]\n",
    "            \n",
    "            tokens1 = set(self.cleaned_data.iloc[idx1]['cleaned_title'].split())\n",
    "            tokens2 = set(self.cleaned_data.iloc[idx2]['cleaned_title'].split())\n",
    "            \n",
    "            intersection = len(tokens1.intersection(tokens2))\n",
    "            union = len(tokens1.union(tokens2))\n",
    "            \n",
    "            return intersection / union if union > 0 else 0\n",
    "        except (KeyError, IndexError):\n",
    "            return 0\n",
    "\n",
    "    def find_matches(self, alpha):\n",
    "        self.matches = set()\n",
    "        if not hasattr(self, '_id_to_row_index'):\n",
    "            self._id_to_row_index = {row['patient_id']: i for i, row in self.cleaned_data.iterrows()}\n",
    "        \n",
    "        for id1, id2 in self.candidate_pairs:\n",
    "            similarity = self.compute_jaccard_similarity(id1, id2)\n",
    "            if similarity > alpha:\n",
    "                self.matches.add((min(id1, id2), max(id1, id2)))\n",
    "        \n",
    "        return self.matches\n",
    "    \n",
    "    # def pivot_clustering(self):\n",
    "    #     adj_list = defaultdict(list)\n",
    "    #     all_ids = set()\n",
    "        \n",
    "    #     for id1, id2 in self.matches:\n",
    "    #         adj_list[id1].append(id2)\n",
    "    #         adj_list[id2].append(id1)\n",
    "    #         all_ids.add(id1)\n",
    "    #         all_ids.add(id2)\n",
    "        \n",
    "    #     remaining_nodes = set(all_ids)\n",
    "    #     clusters = []\n",
    "        \n",
    "    #     while remaining_nodes:\n",
    "    #         pivot = None\n",
    "    #         max_degree = -1\n",
    "    #         for node in remaining_nodes:\n",
    "    #             degree = len(adj_list[node])\n",
    "    #             if degree > max_degree:\n",
    "    #                 max_degree = degree\n",
    "    #                 pivot = node\n",
    "\n",
    "    #         if pivot is None or max_degree == 0:\n",
    "    #             if remaining_nodes:\n",
    "    #                 pivot = next(iter(remaining_nodes))\n",
    "    #                 clusters.append([pivot])\n",
    "    #                 remaining_nodes.remove(pivot)\n",
    "    #             continue\n",
    "            \n",
    "    #         neighbors = adj_list[pivot]\n",
    "    #         cluster = [pivot] + neighbors\n",
    "    #         clusters.append(cluster)\n",
    "\n",
    "    #         for node in cluster:\n",
    "    #             if node in remaining_nodes:\n",
    "    #                 remaining_nodes.remove(node)\n",
    "        \n",
    "    #     self.clusters = clusters\n",
    "    #     return self.clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>PT SEEN BY PT,AMBULATING IN [**Doctor Last Nam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111</td>\n",
       "      <td>Chief Complaint:  respiratory distress\\n   HPI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>333</td>\n",
       "      <td>Admission Date:  [**2137-9-29**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>403</td>\n",
       "      <td>Unit No:  [**Numeric Identifier 63001**]\\nAdmi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>503</td>\n",
       "      <td>Admission Date:  [**2126-11-16**]             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46134</th>\n",
       "      <td>99559</td>\n",
       "      <td>Admission Date:  [**2153-6-11**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46135</th>\n",
       "      <td>99564</td>\n",
       "      <td>Admission Date:  [**2157-1-13**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46136</th>\n",
       "      <td>99650</td>\n",
       "      <td>Admission Date:  [**2156-12-25**]             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46137</th>\n",
       "      <td>99712</td>\n",
       "      <td>Admission Date:  [**2161-10-5**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46138</th>\n",
       "      <td>99913</td>\n",
       "      <td>Admission Date:  [**2141-5-10**]              ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46139 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       patient_id                                              notes\n",
       "0              25  PT SEEN BY PT,AMBULATING IN [**Doctor Last Nam...\n",
       "1             111  Chief Complaint:  respiratory distress\\n   HPI...\n",
       "2             333  Admission Date:  [**2137-9-29**]              ...\n",
       "3             403  Unit No:  [**Numeric Identifier 63001**]\\nAdmi...\n",
       "4             503  Admission Date:  [**2126-11-16**]             ...\n",
       "...           ...                                                ...\n",
       "46134       99559  Admission Date:  [**2153-6-11**]              ...\n",
       "46135       99564  Admission Date:  [**2157-1-13**]              ...\n",
       "46136       99650  Admission Date:  [**2156-12-25**]             ...\n",
       "46137       99712  Admission Date:  [**2161-10-5**]              ...\n",
       "46138       99913  Admission Date:  [**2141-5-10**]              ...\n",
       "\n",
       "[46139 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = x1\n",
    "# truth_labels = y1\n",
    "\n",
    "# edges = [(row['lid'], row['rid']) for _, row in truth_labels.iterrows()]\n",
    "# ground_truth_clusters = []\n",
    "# used_ids = set()\n",
    "\n",
    "# for id1, id2 in edges:\n",
    "#     found_cluster = None\n",
    "#     for cluster in ground_truth_clusters:\n",
    "#         if id1 in cluster or id2 in cluster:\n",
    "#             found_cluster = cluster\n",
    "#             break\n",
    "    \n",
    "#     if found_cluster:\n",
    "#         found_cluster.add(id1)\n",
    "#         found_cluster.add(id2)\n",
    "#     else:\n",
    "#         ground_truth_clusters.append({id1, id2})\n",
    "    \n",
    "#     used_ids.add(id1)\n",
    "#     used_ids.add(id2)\n",
    "\n",
    "# ground_truth_clusters = [list(cluster) for cluster in ground_truth_clusters]\n",
    "\n",
    "pipeline = EntityResolutionPipeline()\n",
    "pipeline.set_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_recall_vs_tau(data, ground_truth_clusters):\n",
    "#     \"\"\"Plot recall of candidate pairs as a function of tau\"\"\"\n",
    "#     pipeline = EntityResolutionPipeline()\n",
    "#     pipeline.set_data(data)\n",
    "#     pipeline.clean_data()\n",
    "#     pipeline.create_blocks()\n",
    "    \n",
    "#     tau_values = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "#     recalls = []\n",
    "    \n",
    "#     for tau in tau_values:\n",
    "#         candidates = pipeline.filter_blocks(tau)\n",
    "#         metrics = evaluate_pairs(candidates, ground_truth_clusters)\n",
    "#         recalls.append(metrics['recall'])\n",
    "    \n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(tau_values, recalls, marker='o', linestyle='-')\n",
    "#     plt.xlabel('τ (Block Size Threshold)')\n",
    "#     plt.ylabel('Recall')\n",
    "#     plt.title('Recall of Candidate Pairs vs. τ')\n",
    "#     plt.grid(True)\n",
    "#     plt.savefig('recall_vs_tau.png')\n",
    "#     plt.show()\n",
    "    \n",
    "#     return pd.DataFrame({'tau': tau_values, 'recall': recalls})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_metrics_vs_alpha(data, ground_truth_clusters, fixed_tau=50):\n",
    "#     \"\"\"Plot precision, recall, and F-measure vs alpha with a fixed tau value\"\"\"\n",
    "#     pipeline = EntityResolutionPipeline()\n",
    "#     pipeline.set_data(data)\n",
    "#     pipeline.clean_data()\n",
    "#     pipeline.create_blocks()\n",
    "#     pipeline.filter_blocks(fixed_tau)\n",
    "    \n",
    "#     alpha_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "#     precisions = []\n",
    "#     recalls = []\n",
    "#     f_scores = []\n",
    "    \n",
    "#     for alpha in alpha_values:\n",
    "#         matches = pipeline.find_matches(alpha)\n",
    "#         metrics = evaluate_pairs(matches, ground_truth_clusters)\n",
    "#         precisions.append(metrics['precision'])\n",
    "#         recalls.append(metrics['recall'])\n",
    "#         f_scores.append(metrics['f_score'])\n",
    "    \n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(alpha_values, precisions, marker='o', linestyle='-', label='Precision')\n",
    "#     plt.plot(alpha_values, recalls, marker='s', linestyle='-', label='Recall')\n",
    "#     plt.plot(alpha_values, f_scores, marker='^', linestyle='-', label='F-measure')\n",
    "#     plt.xlabel('α (Similarity Threshold)')\n",
    "#     plt.ylabel('Score')\n",
    "#     plt.title(f'Precision, Recall, and F-measure vs. α (τ={fixed_tau})')\n",
    "#     plt.grid(True)\n",
    "#     plt.legend()\n",
    "#     plt.savefig('metrics_vs_alpha.png')\n",
    "#     plt.show()\n",
    "    \n",
    "#     return pd.DataFrame({\n",
    "#         'alpha': alpha_values,\n",
    "#         'precision': precisions,\n",
    "#         'recall': recalls,\n",
    "#         'f_score': f_scores\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n--- Question 2a ---\")\n",
    "# recall_data = plot_recall_vs_tau(data, ground_truth_clusters)\n",
    "# print(\"Generated plot of recall vs τ\")\n",
    "# print(\"Recall values:\")\n",
    "# print(recall_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n--- Question 2b ---\")\n",
    "# metrics_data = plot_metrics_vs_alpha(data, ground_truth_clusters)\n",
    "# print(\"Generated plot of precision, recall, and F-measure vs α\")\n",
    "# print(\"Metrics values:\")\n",
    "# print(metrics_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(tau, alpha, clean_the_data =True):  \n",
    "    blocks = defaultdict(list)\n",
    "    candidate_pairs = set()\n",
    "    matches = set()\n",
    "    clusters = []\n",
    "    tau = 50\n",
    "    alpha = 0.7\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pipeline = EntityResolutionPipeline()\n",
    "    data = pd.read_csv('new_patient_notes.csv')\n",
    "    pipeline.set_data(data)\n",
    "    ground_truth = pd.read_csv('new_primary_diagnosis.csv').values.tolist()\n",
    "        \n",
    "    if clean_the_data:\n",
    "        data = pipeline.clean_data()\n",
    "\n",
    "    # Blocking\n",
    "    blocking_start = time.time()\n",
    "    blocks = pipeline.create_blocks()\n",
    "    candidates = pipeline.filter_blocks(tau)\n",
    "    blocking_time = time.time() - blocking_start\n",
    "    print(f\"Blocking completed in {blocking_time:.2f} seconds, generated {len(candidates)} candidate pairs\")\n",
    "    \n",
    "    print(list(candidates)[0])    # Evaluate blocking\n",
    "    # block_metrics = evaluate_pairs(candidates, ground_truth)\n",
    "    # print(f\"  → Blocking metrics:  precision={block_metrics['precision']:.3f},\"\n",
    "    #         f\" recall={block_metrics['recall']:.3f},\"\n",
    "    #         f\" F₁={block_metrics['f_score']:.3f}\\n\")\n",
    "   \n",
    "    # Pair matching\n",
    "    matching_start = time.time()\n",
    "    matches = pipeline.find_matches(alpha)\n",
    "    matching_time = time.time() - matching_start\n",
    "    print(f\"Pair matching completed in {matching_time:.2f} seconds, found {len(matches)} matches\")\n",
    "    print(list(matches)[0])\n",
    "    \n",
    "    # match_metrics = evaluate_pairs(matches, ground_truth)\n",
    "    # print(f\"  → Pair‐matching metrics:  precision={match_metrics['precision']:.3f},\"\n",
    "    #         f\" recall={match_metrics['recall']:.3f},\"\n",
    "    #         f\" F₁={match_metrics['f_score']:.3f}\\n\")\n",
    "    \n",
    "    # Clustering\n",
    "    # clustering_start = time.time()\n",
    "    # edges = [(row['lid'], row['rid']) for _, row in truth_labels.iterrows()]\n",
    "    # ground_truth_clusters = []\n",
    "    # used_ids = set()\n",
    "    # for id1, id2 in edges:\n",
    "    #     found_cluster = None\n",
    "    #     for cluster in ground_truth_clusters:\n",
    "    #         if id1 in cluster or id2 in cluster:\n",
    "    #             found_cluster = cluster\n",
    "    #             break\n",
    "        \n",
    "    #     if found_cluster:\n",
    "    #         found_cluster.add(id1)\n",
    "    #         found_cluster.add(id2)\n",
    "    #     else:\n",
    "    #         ground_truth_clusters.append({id1, id2})\n",
    "        \n",
    "    #     used_ids.add(id1)\n",
    "    #     used_ids.add(id2)\n",
    "\n",
    "    # ground_truth_clusters = [list(cluster) for cluster in ground_truth_clusters]\n",
    "    # clusters = pipeline.pivot_clustering()\n",
    "    # print(f\"Created {len(clusters)} clusters\")\n",
    "    # print(\"Sample clusters:\")\n",
    "    # for i, cluster in enumerate(clusters[:5]):\n",
    "    #     for record_id in cluster[:3]:\n",
    "    #         record_title = pipeline.cleaned_data.loc[pipeline.cleaned_data['id'] == record_id, 'cleaned_title'].iloc[0]\n",
    "    #     if len(cluster) > 3:\n",
    "    #         print(f\"  ... and {len(cluster) - 3} more records\")\n",
    "\n",
    "    # clustering_time = time.time() - clustering_start\n",
    "    # print(f\"Clustering completed in {clustering_time:.2f} seconds, found {len(clusters)} clusters\")\n",
    "    \n",
    "    # # Evaluate clustering\n",
    "    # cluster_pairs = set()\n",
    "    # for cluster in clusters:\n",
    "    #     for i in range(len(cluster)):\n",
    "    #         for j in range(i+1, len(cluster)):\n",
    "    #             id1, id2 = cluster[i], cluster[j]\n",
    "    #             cluster_pairs.add((min(id1, id2), max(id1, id2)))\n",
    "\n",
    "    # cluster_metrics = evaluate_pairs(cluster_pairs, ground_truth_clusters)\n",
    "\n",
    "    # print(f\"  → Clustering metrics:  precision={cluster_metrics['precision']:.3f},\"\n",
    "    #               f\" recall={cluster_metrics['recall']:.3f},\"\n",
    "    #               f\" F₁={cluster_metrics['f_score']:.3f}\\n\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    return {\n",
    "        'blocking': {\n",
    "            # 'precision': block_metrics['precision'],\n",
    "            # 'recall': block_metrics['recall'],\n",
    "            # 'f1': block_metrics['f_score'],\n",
    "            'time': blocking_time,\n",
    "            'candidates': len(candidate_pairs)\n",
    "        },\n",
    "        'matching': {\n",
    "            # 'precision': match_metrics['precision'],\n",
    "            # 'recall': match_metrics['recall'],\n",
    "            # 'f1': match_metrics['f_score'],\n",
    "            'time': matching_time,\n",
    "            'matches': len(matches)\n",
    "        },\n",
    "        # 'clustering': {\n",
    "        #     'precision': cluster_metrics['precision'],\n",
    "        #     'recall': cluster_metrics['recall'],\n",
    "        #     'f1': cluster_metrics['f_score'],\n",
    "        #     'time': clustering_time,\n",
    "        #     'clusters': len(clusters)\n",
    "        # },\n",
    "        'total_time': total_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_cleaning(tau, alpha):\n",
    "#     with_cleaning = run_pipeline(tau, alpha, True)\n",
    "#     without_cleaning = run_pipeline(tau, alpha, False)\n",
    "    \n",
    "#     print(\"\\nComparison:\")\n",
    "#     print(f\"With cleaning - Final F1: {with_cleaning['matching']['f1']:.4f}, Total time: {with_cleaning['total_time']:.2f}s\")\n",
    "#     print(f\"Without cleaning - Final F1: {without_cleaning['matching']['f1']:.4f}, Total time: {without_cleaning['total_time']:.2f}s\")\n",
    "    \n",
    "#     return with_cleaning, without_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 2c & 2d ---\n",
      "Blocking completed in 40.30 seconds, generated 12949510 candidate pairs\n",
      "(81491, 41266)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lj/5mq9mzlx1t58q2x7hr_p8l5c0000gn/T/ipykernel_40782/1864534233.py:20: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  'windows', 'win(?:\\d+)?', 'pro', 'home', 'ultimate',\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Question 2c and 2d\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Question 2c & 2d ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m best_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 33\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m(tau, alpha, clean_the_data)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# block_metrics = evaluate_pairs(candidates, ground_truth)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# print(f\"  → Blocking metrics:  precision={block_metrics['precision']:.3f},\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#         f\" recall={block_metrics['recall']:.3f},\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#         f\" F₁={block_metrics['f_score']:.3f}\\n\")\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Pair matching\u001b[39;00m\n\u001b[1;32m     32\u001b[0m matching_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 33\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_matches\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m matching_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m matching_start\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPair matching completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmatching_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds, found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(matches)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m matches\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[28], line 95\u001b[0m, in \u001b[0;36mEntityResolutionPipeline.find_matches\u001b[0;34m(self, alpha)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id_to_row_index \u001b[38;5;241m=\u001b[39m {row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatient_id\u001b[39m\u001b[38;5;124m'\u001b[39m]: i \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcleaned_data\u001b[38;5;241m.\u001b[39miterrows()}\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m id1, id2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcandidate_pairs:\n\u001b[0;32m---> 95\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_jaccard_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m similarity \u001b[38;5;241m>\u001b[39m alpha:\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatches\u001b[38;5;241m.\u001b[39madd((\u001b[38;5;28mmin\u001b[39m(id1, id2), \u001b[38;5;28mmax\u001b[39m(id1, id2)))\n",
      "Cell \u001b[0;32mIn[28], line 83\u001b[0m, in \u001b[0;36mEntityResolutionPipeline.compute_jaccard_similarity\u001b[0;34m(self, id1, id2)\u001b[0m\n\u001b[1;32m     80\u001b[0m     tokens2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcleaned_data\u001b[38;5;241m.\u001b[39miloc[idx2][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_title\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit())\n\u001b[1;32m     82\u001b[0m     intersection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens1\u001b[38;5;241m.\u001b[39mintersection(tokens2))\n\u001b[0;32m---> 83\u001b[0m     union \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtokens1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens2\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m intersection \u001b[38;5;241m/\u001b[39m union \u001b[38;5;28;01mif\u001b[39;00m union \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mIndexError\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Question 2c and 2d\n",
    "print(\"\\n--- Question 2c & 2d ---\")\n",
    "best_results = run_pipeline(50, 0.7, False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
