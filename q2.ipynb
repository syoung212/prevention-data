{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = pd.read_csv('comprehensive_medical_notes.csv')  \n",
    "y1 = pd.read_csv('new_ground_truth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pairs(pred_pairs, ground_truth_clusters):\n",
    "    true_pairs = set()\n",
    "    for cluster in ground_truth_clusters:\n",
    "        for a in range(len(cluster)):\n",
    "            for b in range(a+1, len(cluster)):\n",
    "                i,j = cluster[a], cluster[b]\n",
    "                true_pairs.add((min(i,j), max(i,j)))\n",
    "\n",
    "    all_pairs = pred_pairs.union(true_pairs)\n",
    "    y_true = [1 if p in true_pairs else 0 for p in all_pairs]\n",
    "    y_pred = [1 if p in pred_pairs  else 0 for p in all_pairs]\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred,\n",
    "        average='binary',\n",
    "        zero_division=0\n",
    "    )\n",
    "    return {'precision': p, 'recall': r, 'f_score': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityResolutionPipeline:\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.cleaned_data = None\n",
    "        self.blocks = defaultdict(list)\n",
    "        self.candidate_pairs = set()\n",
    "        # self.matches = set()\n",
    "        # self.clusters = []\n",
    "        \n",
    "    def set_data(self, data):\n",
    "        self.data = data.copy()\n",
    "        self.cleaned_data = data.copy()\n",
    "        self.cleaned_data['cleaned_title'] = self.cleaned_data['important_notes'].apply(str)\n",
    "        return self.data\n",
    "    \n",
    "    def clean_data(self):\n",
    "        self.cleaned_data = self.data.copy()\n",
    "        extra_words = {\n",
    "            'bundle', 'warranty', 'deal', 'offers?', 'shipping',\n",
    "            'windows', 'win(?:\\d+)?', 'pro', 'home', 'ultimate',\n",
    "            'beats', 'audio'\n",
    "        } \n",
    "        common_words = [\n",
    "            'vology', '-', '/', '\"', 'hdd', 'ssd', 'windows ', 'windows',\n",
    "            'laptop', 'new', 'sale', 'core', 'buy', 'computers', 'computer', \n",
    "            'computers', 'com'\n",
    "        ]\n",
    "        def clean_title(title):\n",
    "            title = title.lower()\n",
    "            title = re.sub(r'[!@#$%^&*()_+={}\\[\\]|\\\\:;<>?,./\"]', ' ', title)\n",
    "            title_tokens = title.split()\n",
    "            title_tokens = [token for token in title_tokens if token not in ENGLISH_STOP_WORDS]\n",
    "            title = ' '.join(title_tokens)\n",
    "            title = re.sub(r'(\\d+)\\s*gb', r'\\1gb', title)  \n",
    "            title = re.sub(r'(\\d+)\\s*tb', r'\\1tb', title) \n",
    "            title = re.sub(r'core\\s*i(\\d)', r'corei\\1', title)  \n",
    "            title = re.sub(r'windows\\s*(\\d+)', r'windows\\1', title) \n",
    "            title = re.sub(r'gigs', 'gb', title)\n",
    "            \n",
    "            for i in common_words:\n",
    "                title = title.replace(i, '')\n",
    "\n",
    "            for i in extra_words:\n",
    "                title = title.replace(i, '')\n",
    "            \n",
    "            return title\n",
    "        \n",
    "        self.cleaned_data['cleaned_title'] = self.cleaned_data['important_notes'].apply(clean_title)\n",
    "        return self.cleaned_data\n",
    "    \n",
    "    def create_blocks(self, max_blocks=5000):    \n",
    "        self.blocks = defaultdict(list)\n",
    "        \n",
    "        for idx, row in self.cleaned_data.iterrows():\n",
    "            tokens = row['cleaned_title'].split()\n",
    "            for token in tokens:\n",
    "                if len(token) > 2:  \n",
    "                    self.blocks[token].append(row['patient_id'])\n",
    "        \n",
    "        return self.blocks\n",
    "\n",
    "    \n",
    "    def filter_blocks(self, tau):\n",
    "        max_blocks=5000\n",
    "        # 1) filter by tau\n",
    "        filtered_blocks = {\n",
    "            token: records\n",
    "            for token, records in self.blocks.items()\n",
    "            if 1 < len(records) < tau\n",
    "        }\n",
    "\n",
    "        # (optional) prune to max_blocks as we discussed before\n",
    "        if max_blocks and len(filtered_blocks) > max_blocks:\n",
    "            tokens_sorted = sorted(\n",
    "                filtered_blocks.keys(),\n",
    "                key=lambda t: len(filtered_blocks[t]),\n",
    "                reverse=True,\n",
    "            )\n",
    "            keep = set(tokens_sorted[:max_blocks])\n",
    "            filtered_blocks = {t: filtered_blocks[t] for t in keep}\n",
    "\n",
    "        # **store** the filtered blocks for later inspection\n",
    "        self.filtered_blocks = filtered_blocks\n",
    "\n",
    "        # 2) build candidate pairs\n",
    "        self.candidate_pairs = set()\n",
    "        for records in filtered_blocks.values():\n",
    "            for i in range(len(records)):\n",
    "                for j in range(i + 1, len(records)):\n",
    "                    self.candidate_pairs.add((records[i], records[j]))\n",
    "\n",
    "        # **return** both, so you can optionally unpack them\n",
    "        return filtered_blocks, self.candidate_pairs\n",
    "\n",
    "    \n",
    "    def compute_jaccard_similarity(self, id1, id2):\n",
    "        if not hasattr(self, '_id_to_row_index'):\n",
    "            self._id_to_row_index = {row['patient_id']: i for i, row in self.cleaned_data.iterrows()}\n",
    "        try:\n",
    "            idx1 = self._id_to_row_index[id1]\n",
    "            idx2 = self._id_to_row_index[id2]\n",
    "            \n",
    "            tokens1 = set(self.cleaned_data.iloc[idx1]['cleaned_title'].split())\n",
    "            tokens2 = set(self.cleaned_data.iloc[idx2]['cleaned_title'].split())\n",
    "            \n",
    "            intersection = len(tokens1.intersection(tokens2))\n",
    "            union = len(tokens1.union(tokens2))\n",
    "            \n",
    "            return intersection / union if union > 0 else 0\n",
    "        except (KeyError, IndexError):\n",
    "            return 0\n",
    "\n",
    "    def find_matches(self, alpha):\n",
    "        self.matches = set()\n",
    "        if not hasattr(self, '_id_to_row_index'):\n",
    "            self._id_to_row_index = {row['patient_id']: i for i, row in self.cleaned_data.iterrows()}\n",
    "        \n",
    "        for id1, id2 in self.candidate_pairs:\n",
    "            similarity = self.compute_jaccard_similarity(id1, id2)\n",
    "            if similarity > alpha:\n",
    "                self.matches.add((min(id1, id2), max(id1, id2)))\n",
    "        \n",
    "        return self.matches\n",
    "    \n",
    "    # def pivot_clustering(self):\n",
    "    #     adj_list = defaultdict(list)\n",
    "    #     all_ids = set()\n",
    "        \n",
    "    #     for id1, id2 in self.matches:\n",
    "    #         adj_list[id1].append(id2)\n",
    "    #         adj_list[id2].append(id1)\n",
    "    #         all_ids.add(id1)\n",
    "    #         all_ids.add(id2)\n",
    "        \n",
    "    #     remaining_nodes = set(all_ids)\n",
    "    #     clusters = []\n",
    "        \n",
    "    #     while remaining_nodes:\n",
    "    #         pivot = None\n",
    "    #         max_degree = -1\n",
    "    #         for node in remaining_nodes:\n",
    "    #             degree = len(adj_list[node])\n",
    "    #             if degree > max_degree:\n",
    "    #                 max_degree = degree\n",
    "    #                 pivot = node\n",
    "\n",
    "    #         if pivot is None or max_degree == 0:\n",
    "    #             if remaining_nodes:\n",
    "    #                 pivot = next(iter(remaining_nodes))\n",
    "    #                 clusters.append([pivot])\n",
    "    #                 remaining_nodes.remove(pivot)\n",
    "    #             continue\n",
    "            \n",
    "    #         neighbors = adj_list[pivot]\n",
    "    #         cluster = [pivot] + neighbors\n",
    "    #         clusters.append(cluster)\n",
    "\n",
    "    #         for node in cluster:\n",
    "    #             if node in remaining_nodes:\n",
    "    #                 remaining_nodes.remove(node)\n",
    "        \n",
    "    #     self.clusters = clusters\n",
    "    #     return self.clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = x1\n",
    "# truth_labels = y1\n",
    "\n",
    "# edges = [(row['lid'], row['rid']) for _, row in truth_labels.iterrows()]\n",
    "# ground_truth_clusters = []\n",
    "# used_ids = set()\n",
    "\n",
    "# for id1, id2 in edges:\n",
    "#     found_cluster = None\n",
    "#     for cluster in ground_truth_clusters:\n",
    "#         if id1 in cluster or id2 in cluster:\n",
    "#             found_cluster = cluster\n",
    "#             break\n",
    "    \n",
    "#     if found_cluster:\n",
    "#         found_cluster.add(id1)\n",
    "#         found_cluster.add(id2)\n",
    "#     else:\n",
    "#         ground_truth_clusters.append({id1, id2})\n",
    "    \n",
    "#     used_ids.add(id1)\n",
    "#     used_ids.add(id2)\n",
    "\n",
    "# ground_truth_clusters = [list(cluster) for cluster in ground_truth_clusters]\n",
    "\n",
    "pipeline = EntityResolutionPipeline()\n",
    "pipeline.set_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_recall_vs_tau(data, ground_truth_clusters):\n",
    "#     \"\"\"Plot recall of candidate pairs as a function of tau\"\"\"\n",
    "#     pipeline = EntityResolutionPipeline()\n",
    "#     pipeline.set_data(data)\n",
    "#     pipeline.clean_data()\n",
    "#     pipeline.create_blocks()\n",
    "    \n",
    "#     tau_values = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "#     recalls = []\n",
    "    \n",
    "#     for tau in tau_values:\n",
    "#         candidates = pipeline.filter_blocks(tau)\n",
    "#         metrics = evaluate_pairs(candidates, ground_truth_clusters)\n",
    "#         recalls.append(metrics['recall'])\n",
    "    \n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(tau_values, recalls, marker='o', linestyle='-')\n",
    "#     plt.xlabel('τ (Block Size Threshold)')\n",
    "#     plt.ylabel('Recall')\n",
    "#     plt.title('Recall of Candidate Pairs vs. τ')\n",
    "#     plt.grid(True)\n",
    "#     plt.savefig('recall_vs_tau.png')\n",
    "#     plt.show()\n",
    "    \n",
    "#     return pd.DataFrame({'tau': tau_values, 'recall': recalls})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_metrics_vs_alpha(data, ground_truth_clusters, fixed_tau=50):\n",
    "#     \"\"\"Plot precision, recall, and F-measure vs alpha with a fixed tau value\"\"\"\n",
    "#     pipeline = EntityResolutionPipeline()\n",
    "#     pipeline.set_data(data)\n",
    "#     pipeline.clean_data()\n",
    "#     pipeline.create_blocks()\n",
    "#     pipeline.filter_blocks(fixed_tau)\n",
    "    \n",
    "#     alpha_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "#     precisions = []\n",
    "#     recalls = []\n",
    "#     f_scores = []\n",
    "    \n",
    "#     for alpha in alpha_values:\n",
    "#         matches = pipeline.find_matches(alpha)\n",
    "#         metrics = evaluate_pairs(matches, ground_truth_clusters)\n",
    "#         precisions.append(metrics['precision'])\n",
    "#         recalls.append(metrics['recall'])\n",
    "#         f_scores.append(metrics['f_score'])\n",
    "    \n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(alpha_values, precisions, marker='o', linestyle='-', label='Precision')\n",
    "#     plt.plot(alpha_values, recalls, marker='s', linestyle='-', label='Recall')\n",
    "#     plt.plot(alpha_values, f_scores, marker='^', linestyle='-', label='F-measure')\n",
    "#     plt.xlabel('α (Similarity Threshold)')\n",
    "#     plt.ylabel('Score')\n",
    "#     plt.title(f'Precision, Recall, and F-measure vs. α (τ={fixed_tau})')\n",
    "#     plt.grid(True)\n",
    "#     plt.legend()\n",
    "#     plt.savefig('metrics_vs_alpha.png')\n",
    "#     plt.show()\n",
    "    \n",
    "#     return pd.DataFrame({\n",
    "#         'alpha': alpha_values,\n",
    "#         'precision': precisions,\n",
    "#         'recall': recalls,\n",
    "#         'f_score': f_scores\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n--- Question 2a ---\")\n",
    "# recall_data = plot_recall_vs_tau(data, ground_truth_clusters)\n",
    "# print(\"Generated plot of recall vs τ\")\n",
    "# print(\"Recall values:\")\n",
    "# print(recall_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n--- Question 2b ---\")\n",
    "# metrics_data = plot_metrics_vs_alpha(data, ground_truth_clusters)\n",
    "# print(\"Generated plot of precision, recall, and F-measure vs α\")\n",
    "# print(\"Metrics values:\")\n",
    "# print(metrics_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_blocks_global = []\n",
    "def run_pipeline(tau, alpha, clean_the_data =True):  \n",
    "    blocks = defaultdict(list)\n",
    "    candidate_pairs = set()\n",
    "    matches = set()\n",
    "    clusters = []\n",
    "    tau = 50\n",
    "    alpha = 0.7\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pipeline = EntityResolutionPipeline()\n",
    "    data = pd.read_csv('compiled_important_notes.csv')\n",
    "    pipeline.set_data(data)\n",
    "    ground_truth = pd.read_csv('new_primary_diagnosis.csv').values.tolist()\n",
    "        \n",
    "    if clean_the_data:\n",
    "        data = pipeline.clean_data()\n",
    "\n",
    "    # Blocking\n",
    "    blocking_start = time.time()\n",
    "    # candidates = pipeline.filter_blocks(tau)\n",
    "    # blocking_time = time.time() - blocking_start\n",
    "    # print(f\"Blocking completed in {blocking_time:.2f} seconds, generated {len(candidates)} candidate pairs\")\n",
    "    \n",
    "    # print(list(candidates)[0])    \n",
    "\n",
    "    blocks = pipeline.create_blocks()\n",
    "    filtered_blocks, candidates = pipeline.filter_blocks(tau)\n",
    "    filtered_blocks_global = filtered_blocks\n",
    "\n",
    "    print(f\"Kept {len(filtered_blocks)} blocks; generated {len(candidates)} candidate pairs\")\n",
    "\n",
    "    # inspect the *first* filtered block:\n",
    "    first_token, first_records = next(iter(filtered_blocks.items()))\n",
    "    print(\"First block token:\", first_token)\n",
    "    print(\"Records in that block:\", first_records)\n",
    "    blocking_time = time.time() - blocking_start\n",
    "    \n",
    "\n",
    "    # Evaluate blocking\n",
    "    # block_metrics = evaluate_pairs(candidates, ground_truth)\n",
    "    # print(f\"  → Blocking metrics:  precision={block_metrics['precision']:.3f},\"\n",
    "    #         f\" recall={block_metrics['recall']:.3f},\"\n",
    "    #         f\" F₁={block_metrics['f_score']:.3f}\\n\")\n",
    "\n",
    "\n",
    "   \n",
    "    # Pair matching\n",
    "    matching_start = time.time()\n",
    "    matches = pipeline.find_matches(alpha)\n",
    "    matching_time = time.time() - matching_start\n",
    "    print(f\"Pair matching completed in {matching_time:.2f} seconds, found {len(matches)} matches\")\n",
    "    print(list(matches)[0])\n",
    "\n",
    "\n",
    "    \n",
    "    # match_metrics = evaluate_pairs(matches, ground_truth)\n",
    "    # print(f\"  → Pair‐matching metrics:  precision={match_metrics['precision']:.3f},\"\n",
    "    #         f\" recall={match_metrics['recall']:.3f},\"\n",
    "    #         f\" F₁={match_metrics['f_score']:.3f}\\n\")\n",
    "    \n",
    "    # Clustering\n",
    "    # clustering_start = time.time()\n",
    "    # edges = [(row['lid'], row['rid']) for _, row in truth_labels.iterrows()]\n",
    "    # ground_truth_clusters = []\n",
    "    # used_ids = set()\n",
    "    # for id1, id2 in edges:\n",
    "    #     found_cluster = None\n",
    "    #     for cluster in ground_truth_clusters:\n",
    "    #         if id1 in cluster or id2 in cluster:\n",
    "    #             found_cluster = cluster\n",
    "    #             break\n",
    "        \n",
    "    #     if found_cluster:\n",
    "    #         found_cluster.add(id1)\n",
    "    #         found_cluster.add(id2)\n",
    "    #     else:\n",
    "    #         ground_truth_clusters.append({id1, id2})\n",
    "        \n",
    "    #     used_ids.add(id1)\n",
    "    #     used_ids.add(id2)\n",
    "\n",
    "    # ground_truth_clusters = [list(cluster) for cluster in ground_truth_clusters]\n",
    "    # clusters = pipeline.pivot_clustering()\n",
    "    # print(f\"Created {len(clusters)} clusters\")\n",
    "    # print(\"Sample clusters:\")\n",
    "    # for i, cluster in enumerate(clusters[:5]):\n",
    "    #     for record_id in cluster[:3]:\n",
    "    #         record_title = pipeline.cleaned_data.loc[pipeline.cleaned_data['id'] == record_id, 'cleaned_title'].iloc[0]\n",
    "    #     if len(cluster) > 3:\n",
    "    #         print(f\"  ... and {len(cluster) - 3} more records\")\n",
    "\n",
    "    # clustering_time = time.time() - clustering_start\n",
    "    # print(f\"Clustering completed in {clustering_time:.2f} seconds, found {len(clusters)} clusters\")\n",
    "    \n",
    "    # # Evaluate clustering\n",
    "    # cluster_pairs = set()\n",
    "    # for cluster in clusters:\n",
    "    #     for i in range(len(cluster)):\n",
    "    #         for j in range(i+1, len(cluster)):\n",
    "    #             id1, id2 = cluster[i], cluster[j]\n",
    "    #             cluster_pairs.add((min(id1, id2), max(id1, id2)))\n",
    "\n",
    "    # cluster_metrics = evaluate_pairs(cluster_pairs, ground_truth_clusters)\n",
    "\n",
    "    # print(f\"  → Clustering metrics:  precision={cluster_metrics['precision']:.3f},\"\n",
    "    #               f\" recall={cluster_metrics['recall']:.3f},\"\n",
    "    #               f\" F₁={cluster_metrics['f_score']:.3f}\\n\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    return {\n",
    "        'blocking': {\n",
    "            # 'precision': block_metrics['precision'],\n",
    "            # 'recall': block_metrics['recall'],\n",
    "            # 'f1': block_metrics['f_score'],\n",
    "            'time': blocking_time,\n",
    "            'candidates': len(candidate_pairs)\n",
    "        },\n",
    "        'matching': {\n",
    "            # 'precision': match_metrics['precision'],\n",
    "            # 'recall': match_metrics['recall'],\n",
    "            # 'f1': match_metrics['f_score'],\n",
    "            'time': matching_time,\n",
    "            'matches': len(matches)\n",
    "        },\n",
    "        # 'clustering': {\n",
    "        #     'precision': cluster_metrics['precision'],\n",
    "        #     'recall': cluster_metrics['recall'],\n",
    "        #     'f1': cluster_metrics['f_score'],\n",
    "        #     'time': clustering_time,\n",
    "        #     'clusters': len(clusters)\n",
    "        # },\n",
    "        'total_time': total_time, \n",
    "        'filtered_blocks_global': filtered_blocks_global\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_cleaning(tau, alpha):\n",
    "#     with_cleaning = run_pipeline(tau, alpha, True)\n",
    "#     without_cleaning = run_pipeline(tau, alpha, False)\n",
    "    \n",
    "#     print(\"\\nComparison:\")\n",
    "#     print(f\"With cleaning - Final F1: {with_cleaning['matching']['f1']:.4f}, Total time: {with_cleaning['total_time']:.2f}s\")\n",
    "#     print(f\"Without cleaning - Final F1: {without_cleaning['matching']['f1']:.4f}, Total time: {without_cleaning['total_time']:.2f}s\")\n",
    "    \n",
    "#     return with_cleaning, without_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2c and 2d\n",
    "print(\"\\n--- Question 2c & 2d ---\")\n",
    "best_results = run_pipeline(50, 0.7, False)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read your main data\n",
    "df = pd.read_csv('compiled_important_notes.csv')\n",
    "\n",
    "# Initialize a list to hold result rows\n",
    "block_rows = []\n",
    "filtered_blocks_global = best_results['filtered_blocks_global']\n",
    "\n",
    "# Iterate through all blocks\n",
    "for token, patient_ids in filtered_blocks_global.items():\n",
    "    # Filter the dataframe to just patients in this block\n",
    "    matches = df[df['patient_id'].isin(patient_ids)]\n",
    "\n",
    "    # Aggregate the first 5 notes per patient_id\n",
    "    agg_dict = (\n",
    "        matches\n",
    "        .groupby('patient_id')['important_notes']\n",
    "        .apply(lambda notes: ' '.join(notes.head(5)))\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # Build the strings\n",
    "    block_list_str = ','.join(map(str, patient_ids))\n",
    "    all_notes_list = [agg_dict[pid] for pid in patient_ids if pid in agg_dict]\n",
    "    all_notes_str = ' '.join(all_notes_list)\n",
    "\n",
    "    # Save the row\n",
    "    block_rows.append({\n",
    "        'block_list_str': block_list_str,\n",
    "        'all_notes': all_notes_str\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "result_df = pd.DataFrame(block_rows)\n",
    "\n",
    "# Save to CSV\n",
    "result_df.to_csv('filtered_blocks_notes.csv', index=False)\n",
    "\n",
    "# Optional: Show preview\n",
    "print(result_df.head().to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load your master list of patient IDs\n",
    "all_patients = pd.read_csv('compiled_important_notes.csv')['patient_id'].astype(int)\n",
    "\n",
    "test_ids = all_patients.sample(frac=0.03, random_state=50).tolist()\n",
    "# Or sample a fixed number: test_ids = all_patients.sample(n=100, random_state=42).tolist()\n",
    "\n",
    "# 3) Load the precomputed blocks + notes CSV\n",
    "blocks_df = pd.read_csv('filtered_blocks_notes.csv')\n",
    "blocks_df['patient_list'] = blocks_df['block_list_str'].str.split(',')\n",
    "\n",
    "# 4) Explode to one row per patient\n",
    "exploded = (\n",
    "    blocks_df\n",
    "    .explode('patient_list')\n",
    "    .rename(columns={'patient_list': 'patient_id'})\n",
    ")\n",
    "exploded['patient_id'] = exploded['patient_id'].astype(int)\n",
    "\n",
    "# 5) Filter to just your test IDs\n",
    "test_set = exploded[exploded['patient_id'].isin(test_ids)][\n",
    "    ['patient_id', 'block_list_str', 'all_notes']\n",
    "].drop_duplicates('patient_id').reset_index(drop=True)\n",
    "\n",
    "# 6) Save your test set for model evaluation\n",
    "test_set.to_csv('er_test_set.csv', index=False)\n",
    "\n",
    "print(f\"Saved {len(test_set)} rows to er_test_set.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "API_KEY = ''\n",
    "API_URL = 'https://api.perplexity.ai/chat/completions'\n",
    "MODEL   = 'llama-3.1-sonar-large-128k-online'\n",
    "INPUT_CSV  = 'er_test_set.csv'\n",
    "OUTPUT_CSV = 'er_test_icd9_codes.csv'\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "def diagnose_icd9(notes: str) -> str:\n",
    "    system_msg = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a board‑certified medical coder. It is crucial that you are providing the right range in IC9 codes.\"\n",
    "            \"Return exactly one ICD‑9 code for these clinical notes. Only include the code and no other words with nothing following or before it.\"\n",
    "        )\n",
    "    }\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": notes\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [system_msg, user_msg],\n",
    "        \"max_tokens\": 10,\n",
    "        \"temperature\": 0.0,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    resp = requests.post(API_URL, json=payload, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    content = resp.json()['choices'][0]['message']['content'].strip()\n",
    "    integer_code = content.split('.')[0]\n",
    "    return integer_code\n",
    "\n",
    "results = []\n",
    "for _, row in df.iterrows():\n",
    "    pid   = row['patient_id']\n",
    "    notes = row['all_notes']\n",
    "    try:\n",
    "        code = diagnose_icd9(notes)\n",
    "    except Exception:\n",
    "        code = None\n",
    "    results.append({\"patient_id\": pid, \"icd9_code\": code})\n",
    "\n",
    "# Save only patient_id + icd9_code\n",
    "out_df = pd.DataFrame(results)\n",
    "out_df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "# And if you just want to print the list in Python:\n",
    "print(out_df.values.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load ER data\n",
    "er_df = pd.read_csv(\"er_test_icd9_codes.csv\")\n",
    "\n",
    "# Load ground truth (assuming it's a CSV as well)\n",
    "ground_truth_df = pd.read_csv(\"new_ground_truth.csv\")\n",
    "\n",
    "import ast\n",
    "\n",
    "def safe_eval(val):\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "ground_truth_df['diagnosis'] = ground_truth_df['diagnosis'].apply(safe_eval)\n",
    "\n",
    "\n",
    "# Merge ER data with ground truth on patient_id\n",
    "merged_df = pd.merge(er_df, ground_truth_df, on='patient_id', how='left')\n",
    "\n",
    "# Convert ICD9 code to string to match diagnosis format\n",
    "merged_df['icd9_code'] = merged_df['icd9_code'].astype(str)\n",
    "\n",
    "def is_code_in_same_range(icd9_code, diagnosis_list):\n",
    "    try:\n",
    "        icd9_int = int(float(icd9_code))  # handles cases like \"438.1\"\n",
    "        icd9_range = icd9_int // 100\n",
    "        for diag in diagnosis_list:\n",
    "            try:\n",
    "                diag_int = int(float(diag))\n",
    "                if diag_int // 100 == icd9_range:\n",
    "                    return True\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "        return False\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "merged_df['is_match'] = merged_df.apply(\n",
    "    lambda row: is_code_in_same_range(row['icd9_code'], row['diagnosis']) if isinstance(row['diagnosis'], list) else False,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = merged_df['is_match'].mean()\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(merged_df[['patient_id', 'icd9_code', 'diagnosis', 'is_match']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
