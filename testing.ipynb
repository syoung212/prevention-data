{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install --upgrade google-cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install db_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-bigquery\n",
    "\n",
    "!gcloud --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth list\n",
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"exp-dl-sy398\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import db_dtypes as db_dtypes\n",
    "\n",
    "client = bigquery.Client(project=project_name)\n",
    "\n",
    "# List datasets in the specified project\n",
    "datasets = list(client.list_datasets())\n",
    "\n",
    "# Print the dataset names\n",
    "for dataset in datasets:\n",
    "    print(dataset.dataset_id)\n",
    "\n",
    "# TODO(developer): Set table_id to the ID of the destination table.\n",
    "# table_id = \"your-project.your_dataset.your_table_name\"\n",
    "sql = f'SELECT * FROM `{project_name}.mimic3_v1_4.PATIENTS`'\n",
    "query_job = client.query(sql)\n",
    "\n",
    "# df = query_job.to_dataframe()\n",
    "rows = query_job.result()\n",
    "patients_df = pd.DataFrame([dict(row) for row in rows])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "patients_df['GENDER'].value_counts().plot(kind='bar')\n",
    "\n",
    "plt.title('Gender Distribution in MIMIC-III Patient Dataset')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitarray\n",
    "from bitarray import bitarray\n",
    "import hashlib\n",
    "import random\n",
    "import math\n",
    "random.seed(0)\n",
    "\n",
    "class BloomFilter(object):\n",
    "    def __init__(self, size, hash_count):\n",
    "        \"\"\"\n",
    "        size: size of bit array\n",
    "        hash_count: number of hash functions to use\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.hash_count = hash_count\n",
    "        self.bit_array = bitarray(size)\n",
    "        self.bit_array.setall(0)\n",
    "        self.hash_param = []\n",
    "        i=0\n",
    "        while i<hash_count:\n",
    "            a=random.randint(1,9999)\n",
    "            b=random.randint(1,9999)\n",
    "            p = self.generate_large_prime(30)\n",
    "            self.hash_param.append((a,b,p))\n",
    "            i+=1\n",
    "            \n",
    "    def generate_large_prime(self, bit_size):\n",
    "\n",
    "        random_number = random.getrandbits(bit_size)\n",
    "        \n",
    "        while not self.is_prime(random_number):\n",
    "            random_number = random.getrandbits(bit_size)\n",
    "        return random_number\n",
    "\n",
    "    def is_prime(self, number):\n",
    "        if number % 2 == 0:\n",
    "            return False\n",
    "\n",
    "\n",
    "        for i in range(3, int(math.sqrt(number)) + 1, 2):\n",
    "            if number % i == 0:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "    def calculate_hash(self,item,hash_params):\n",
    "        item_val = (hash_params[0]*item + hash_params[1])%hash_params[2]\n",
    "        #print (item_val,hash_params[2])\n",
    "        return item_val\n",
    "    \n",
    "    \n",
    "    def add(self, item):\n",
    "        \"\"\"\n",
    "        Add an item to the filter\n",
    "        \"\"\"\n",
    "        for p in self.hash_param:\n",
    "            self.bit_array[self.calculate_hash(item, p)% self.size] = 1\n",
    "        \n",
    "    def lookup(self, item):\n",
    "        \"\"\"\n",
    "        Check for existence of an item in filter\n",
    "        \"\"\"\n",
    "\n",
    "        for p in self.hash_param:\n",
    "            if self.bit_array[self.calculate_hash(item, p)% self.size] == 0:\n",
    "                return False\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to join the admissions table with the diagnoses table\n",
    "sql = f\"\"\"\n",
    "SELECT\n",
    "    a.SUBJECT_ID,\n",
    "    a.HADM_ID,\n",
    "    d.ICD9_CODE,\n",
    "    a.ADMITTIME\n",
    "FROM\n",
    "    `{project_name}.mimic3_v1_4.ADMISSIONS` as a\n",
    "LEFT JOIN\n",
    "    `{project_name}.mimic3_v1_4.DIAGNOSES_ICD` as d\n",
    "ON\n",
    "    a.SUBJECT_ID = d.SUBJECT_ID AND a.HADM_ID = d.HADM_ID\n",
    "\"\"\"\n",
    "query_job = client.query(sql)\n",
    "rows = query_job.result()\n",
    "\n",
    "diagnoses_df = pd.DataFrame([dict(row) for row in rows])\n",
    "\n",
    "diagnoses_df['DIAGNOSIS_FLAG'] = diagnoses_df['ICD9_CODE'].notnull()\n",
    "\n",
    "# Left join because want to keep all admissions to the hospital, and add false to flag if no diagnosis was given\n",
    "\n",
    "ground_truth_df = diagnoses_df[['SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'ICD9_CODE', 'DIAGNOSIS_FLAG']]\n",
    "\n",
    "ground_truth_df['DIAGNOSIS_FLAG'].fillna(False, inplace=True)\n",
    "print(\"Ground Truth DataFrame:\", len(ground_truth_df))\n",
    "print(ground_truth_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each ICD9_CODE\n",
    "icd9_counts = diagnoses_df['ICD9_CODE'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "icd9_counts.head(20).plot(kind='bar')\n",
    "\n",
    "plt.title('Top 20 Most Common ICD-9 Diagnoses')\n",
    "plt.xlabel('ICD-9 Code')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd_d_sql = f'SELECT * FROM `{project_name}.mimic3_v1_4.D_ICD_DIAGNOSES`'\n",
    "icd_df_job = client.query(icd_d_sql)\n",
    "icd_df_rows = icd_df_job.result()\n",
    "icd_df = pd.DataFrame([dict(row) for row in icd_df_rows])\n",
    "icd_df = icd_df.dropna(how='all')\n",
    "\n",
    "ground_truth_df = pd.merge(diagnoses_df, icd_df, on=['ICD9_CODE'], how='left')\n",
    "print(ground_truth_df.head(5))\n",
    "\n",
    "icd9_counts = ground_truth_df['SHORT_TITLE'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "icd9_counts.head(20).plot(kind='bar')\n",
    "\n",
    "plt.title('Top 20 Most Common ICD-9 Diagnoses')\n",
    "plt.xlabel('ICD-9 Code')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patients_df = patients_df.dropna(how='all')\n",
    "admissions_sql = f'SELECT * FROM `{project_name}.mimic3_v1_4.ADMISSIONS`'\n",
    "admissions_query_job = client.query(admissions_sql)\n",
    "admissions_rows = admissions_query_job.result()\n",
    "admissions_df = pd.DataFrame([dict(row) for row in admissions_rows])\n",
    "admissions_df = admissions_df.dropna(how='all')\n",
    "print(admissions_df['SUBJECT_ID'].isin(patients_df['SUBJECT_ID']).all())  # True means filtering is unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloom Filter Testing\n",
    "\n",
    "bloom_filter = BloomFilter(size=100000, hash_count=5)\n",
    "\n",
    "for subject_id in patients_df['SUBJECT_ID']:\n",
    "    bloom_filter.add(subject_id)\n",
    "\n",
    "filtered_admissions_df = admissions_df[admissions_df['SUBJECT_ID'].apply(\n",
    "    lambda x: bloom_filter.lookup(x))]\n",
    "\n",
    "result_df = pd.merge(patients_df, filtered_admissions_df, on='SUBJECT_ID', how='inner')\n",
    "\n",
    "print(\"Patients DataFrame length\", len(patients_df))\n",
    "print(\"Admissions DataFrame length\", len(admissions_df))\n",
    "print(\"Filtered DataFrame length\", len(filtered_admissions_df))\n",
    "print(\"Patient-Admissions DataFrame length\", len(result_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloom Filter Testing with Diagnoses Table\n",
    "diagnoses_sql = f\"SELECT * FROM `{project_name}.mimic3_v1_4.DIAGNOSES_ICD`\"\n",
    "diagnoses = client.query(diagnoses_sql)\n",
    "diagnoses_rows = diagnoses.result()\n",
    "diagnoses_icd_df = pd.DataFrame([dict(row) for row in diagnoses_rows])\n",
    "print(admissions_df['SUBJECT_ID'].isin(diagnoses_icd_df['SUBJECT_ID']).all())  # True means filtering is unnecessary\n",
    "print(patients_df['SUBJECT_ID'].isin(diagnoses_icd_df['SUBJECT_ID']).all())  # True means filtering is unnecessary\n",
    "diagnoses_icd_df = diagnoses_icd_df.dropna(how='all')\n",
    "\n",
    "print(\"Diagnoses DataFrame length\", len(diagnoses_icd_df))\n",
    "print(\"Patient-Admissions DataFrame length\", len(result_df))\n",
    "\n",
    "filtered_admissions_df = diagnoses_icd_df[diagnoses_icd_df['SUBJECT_ID'].apply(\n",
    "    lambda x: bloom_filter.lookup(x))]\n",
    "\n",
    "print(\"Filtered Admissions DataFrame length\", len(filtered_admissions_df))\n",
    "result_df = pd.merge(patients_df, diagnoses_icd_df, on='SUBJECT_ID', how='inner')\n",
    "\n",
    "print(\"Patient-Admissions-Diagnoses DataFrame length\", len(result_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeevents_sql = f\"\"\"\n",
    "SELECT SUBJECT_ID, HADM_ID, ITEMID, VALUE, VALUEUOM, FLAG  \n",
    "FROM `{project_name}.mimic3_v1_4.LABEVENTS`  \n",
    "WHERE ITEMID IN (50983, 50971, 50912, 51006, 50907, 50909, 50906, 50954, 51000, 50960, 50902, 50809, 50910, 50970, 50893, 51274, 51237, 50882)  \n",
    "AND DATE(CHARTTIME) >= DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR)\n",
    "AND RAND() <= 0.1;\n",
    "\"\"\"\n",
    "labeevents_job = client.query(labeevents_sql)\n",
    "labeevents_rows = labeevents_job.result()\n",
    "labeevents_df = pd.DataFrame([dict(row) for row in labeevents_rows])\n",
    "labeevents_df = labeevents_df.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses_sql = f'SELECT * FROM `{project_name}.mimic3_v1_4.DIAGNOSES_ICD`'\n",
    "diag_job = client.query(diagnoses_sql)\n",
    "diag_rows = diag_job.result()\n",
    "diag_df = pd.DataFrame([dict(row) for row in diag_rows])\n",
    "diag_df = diag_df.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#medical history experimentation\n",
    "medical_history_df = diagnoses_df.merge(admissions_df[['SUBJECT_ID', 'HADM_ID', 'ADMITTIME']], on=['SUBJECT_ID', 'HADM_ID'])\n",
    "medical_history_df['ADMITTIME'] = pd.to_datetime(medical_history_df['ADMITTIME'])\n",
    "\n",
    "#we want to only count the number of past diagnoses for each patient\n",
    "medical_history_df = medical_history_df.groupby('SUBJECT_ID').apply(\n",
    "    lambda group: group[group['ADMITTIME'] < group['ADMITTIME'].max()]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "past_diagnoses_count = medical_history_df.groupby('SUBJECT_ID')['ICD9_CODE'].nunique().reset_index()\n",
    "past_diagnoses_count.rename(columns={'ICD9_CODE': 'TOTAL_PAST_DIAGNOSES'}, inplace=True)\n",
    "\n",
    "# if there were no previous diagnoses, change the NA to 0\n",
    "patients_medical_df = pd.merge(result_df, past_diagnoses_count, on='SUBJECT_ID', how='left')\n",
    "patients_medical_df['TOTAL_PAST_DIAGNOSES'].fillna(0, inplace=True) \n",
    "\n",
    "# add in lab events\n",
    "feature_df = pd.merge(patients_medical_df, labeevents_df[['HADM_ID', 'ITEMID', 'VALUE']], \n",
    "                            on='HADM_ID', how='left')\n",
    "\n",
    "\n",
    "#final joined dataset for feature analysis\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# import os\n",
    "# import openai\n",
    "# from openai import OpenAI\n",
    "\n",
    "# api_key_ai = os.getenv('OPENAI_API_KEY')\n",
    "# openai.api_key = api_key_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import time\n",
    "\n",
    "# # Set your API key\n",
    "# openai.api_key = \"Enter your OpenAI API key here\"\n",
    "\n",
    "# # Simple function to create a prompt from patient data\n",
    "# def get_prompt(row):\n",
    "#     prompt = f\"\"\"\n",
    "#     Patient information:\n",
    "#     - Age: {row.get('AGE', 'Unknown')}\n",
    "#     - Gender: {row.get('GENDER', 'Unknown')}\n",
    "#     - Past diagnoses: {int(row['TOTAL_PAST_DIAGNOSES'])}\n",
    "#     - Death Flag: {'Yes' if row.get('DOD', None) is not None else 'No'}\n",
    "    \n",
    "#     Lab results:\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # can use itemid if that is provided\n",
    "#     if 'ITEMID' in row and 'VALUE' in row:\n",
    "#         prompt += f\"- Lab {row['ITEMID']}: {row['VALUE']}\\n\"\n",
    "    \n",
    "#     prompt += \"\\nWhat is the most likely diagnosis? Include ICD9 code.\"\n",
    "#     return prompt\n",
    "\n",
    "# def get_diagnosis(prompt):\n",
    "#     response = openai.chat.completions.create(\n",
    "#         model=\"gpt-4\",\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are a medical expert. Provide diagnoses with ICD9 codes.\"},\n",
    "#             {\"role\": \"user\", \"content\": prompt}\n",
    "#         ],\n",
    "#     )\n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "# # Sample a few rows to test\n",
    "# sample = feature_df.sample(3)\n",
    "# results = []\n",
    "\n",
    "# for _, row in sample.iterrows():\n",
    "#     prompt = get_prompt(row)\n",
    "    \n",
    "#     diagnosis = get_diagnosis(prompt)\n",
    "#     results.append({\n",
    "#         'HADM_ID': row.get('HADM_ID', 'Unknown'),\n",
    "#         'predicted_diagnosis': diagnosis\n",
    "#     })\n",
    "\n",
    "# results_df = pd.DataFrame(results)\n",
    "# print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pd.set_option('display.max_colwidth', None)\n",
    "# print(results_df['predicted_diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = 249\n",
    "\n",
    "note_events_sql = f\"\"\"\n",
    "SELECT SUBJECT_ID, HADM_ID, DESCRIPTION, ISERROR, TEXT, CHARTDATE\n",
    "FROM (\n",
    "  SELECT *, ROW_NUMBER() OVER (PARTITION BY SUBJECT_ID ORDER BY CHARTDATE DESC) AS rn\n",
    "  FROM `{project_name}.mimic3_v1_4.NOTEEVENTS`\n",
    "  WHERE SUBJECT_ID = {patient_id}\n",
    ")\n",
    "WHERE rn = 1\n",
    "\"\"\"\n",
    "\n",
    "single_note_job = client.query(note_events_sql)\n",
    "single_note_rows = single_note_job.result()\n",
    "single_note_df = pd.DataFrame([dict(row) for row in single_note_rows])\n",
    "single_note_df = single_note_df.dropna(how='all')\n",
    "print(single_note_df.head(5))\n",
    "print(len(single_note_df))\n",
    "print(single_note_df['TEXT'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_events_sql = f\"\"\"\n",
    "SELECT SUBJECT_ID, HADM_ID, DESCRIPTION, ISERROR, TEXT, CHARTDATE\n",
    "FROM (\n",
    "  SELECT *, ROW_NUMBER() OVER (PARTITION BY SUBJECT_ID ORDER BY CHARTDATE DESC) AS rn\n",
    "  FROM `{project_name}.mimic3_v1_4.NOTEEVENTS`\n",
    "  WHERE HADM_ID IS NOT NULL\n",
    ")\n",
    "WHERE rn = 1\n",
    "\"\"\"\n",
    "\n",
    "note_job = client.query(note_events_sql)\n",
    "note_rows = note_job.result()\n",
    "note_df = pd.DataFrame([dict(row) for row in note_rows])\n",
    "note_df = note_df.dropna(how='all')\n",
    "print(note_df.head(5))\n",
    "print(len(note_df))\n",
    "print(note_df['TEXT'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_df_small = note_df[['SUBJECT_ID', 'HADM_ID', 'TEXT']]\n",
    "\n",
    "diag_df = ground_truth_df[['SUBJECT_ID', 'HADM_ID', 'SHORT_TITLE']]\n",
    "\n",
    "diag_grouped = (\n",
    "    diag_df\n",
    "    .groupby(['SUBJECT_ID', 'HADM_ID'])['SHORT_TITLE']\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "test_ground_truth = pd.merge(\n",
    "    note_df_small,\n",
    "    diag_grouped,\n",
    "    on=['SUBJECT_ID', 'HADM_ID'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(test_ground_truth.head())\n",
    "print(f\"Final merged dataset size: {len(test_ground_truth)}\")\n",
    "\n",
    "one_diagnosis = test_ground_truth\n",
    "one_diagnosis['PRIMARY_DIAGNOSIS'] = one_diagnosis['SHORT_TITLE'].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "print(one_diagnosis.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "filename = 'new_patient_notes.csv'\n",
    "header = ['patient_id', 'notes']\n",
    "rows = note_df[['SUBJECT_ID', 'TEXT']].values.tolist()\n",
    "\n",
    "with open(filename, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    \n",
    "    csvwriter.writerow(header)\n",
    "    csvwriter.writerows(rows)\n",
    "\n",
    "print(f\"Created CSV file: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of entries in CSV file\", len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking that all subject ids from notes_df are also in the ground truth\n",
    "\n",
    "ids_ground_truth = set(test_ground_truth['SUBJECT_ID'])\n",
    "ids_notes = set(note_df['SUBJECT_ID'])\n",
    "\n",
    "missing_subject_ids = ids_ground_truth - ids_notes\n",
    "\n",
    "print(\"Missing Patients\", missing_subject_ids, len(missing_subject_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "filename = 'new_ground_truth.csv'\n",
    "header = ['patient_id', 'diagnosis']\n",
    "rows = test_ground_truth[['SUBJECT_ID', 'SHORT_TITLE']].values.tolist()\n",
    "\n",
    "with open(filename, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    \n",
    "    csvwriter.writerow(header)\n",
    "    csvwriter.writerows(rows)\n",
    "\n",
    "print(f\"Created CSV file: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "filename = 'new_primary_diagnosis.csv'\n",
    "header = ['patient_id', 'diagnosis']\n",
    "rows = one_diagnosis[['SUBJECT_ID', 'PRIMARY_DIAGNOSIS']].values.tolist()\n",
    "\n",
    "with open(filename, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    \n",
    "    csvwriter.writerow(header)\n",
    "    csvwriter.writerows(rows)\n",
    "\n",
    "print(f\"Created CSV file: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"patient_notes.csv\")\n",
    "\n",
    "# Based on the pattern tests, implement the corrected extraction\n",
    "# Helper function for safer extraction with better whitespace handling\n",
    "def safe_extract(text, pattern, flags=re.IGNORECASE, default=None):\n",
    "    if pd.isna(text):\n",
    "        return default\n",
    "    match = re.search(pattern, text, flags=flags)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return default\n",
    "\n",
    "# Helper function for multi-line extraction with improved pattern handling\n",
    "def extract_section(text, pattern, flags=re.IGNORECASE|re.DOTALL, default=None):\n",
    "    if pd.isna(text):\n",
    "        return default\n",
    "    match = re.search(pattern, text, flags=flags)\n",
    "    if match:\n",
    "        result = match.group(1).strip()\n",
    "        # Clean up whitespace\n",
    "        result = re.sub(r'\\s+', ' ', result)\n",
    "        return result\n",
    "    return default\n",
    "\n",
    "# Extract \"Reason:\" using exact pattern matching from tests\n",
    "df[\"reason_simple\"] = df[\"notes\"].apply(\n",
    "    lambda x: safe_extract(x, r\"Reason:[ \\t]*([^\\n]+)\")\n",
    ")\n",
    "\n",
    "# Extract \"REASON FOR THIS EXAMINATION:\" block\n",
    "df[\"reason_block\"] = df[\"notes\"].apply(\n",
    "    lambda x: extract_section(x, r\"REASON FOR THIS EXAMINATION:[ \\t]*\\n([\\s\\S]*?)(?=\\n\\s*\\n|__+)\")\n",
    ")\n",
    "\n",
    "# Extract \"Admitting Diagnosis:\"\n",
    "df[\"admit_dx\"] = df[\"notes\"].apply(\n",
    "    lambda x: safe_extract(x, r\"Admitting Diagnosis:[ \\t]*([^\\n]+)\", re.IGNORECASE)\n",
    ")\n",
    "\n",
    "# Extract PMH section - try both single line and multi-line patterns\n",
    "df[\"pmh_section\"] = df[\"notes\"].apply(\n",
    "    lambda x: (\n",
    "        safe_extract(x, r\"PMH:[ \\t]*([^\\n]+)\", re.IGNORECASE) or \n",
    "        extract_section(x, r\"PMH:[ \\t]*\\n([\\s\\S]*?)(?=\\n\\s*\\n|\\n[A-Z])\", re.IGNORECASE|re.DOTALL)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Extract INDICATION\n",
    "df[\"indication\"] = df[\"notes\"].apply(\n",
    "    lambda x: extract_section(x, r\"INDICATION:[ \\t]*\\n?([\\s\\S]*?)(?=\\n\\s*\\n|TECHNIQUE|COMPARISON)\", re.IGNORECASE|re.DOTALL)\n",
    ")\n",
    "\n",
    "# Find all H/O history items\n",
    "df[\"history_list\"] = df[\"notes\"].apply(\n",
    "    lambda x: re.findall(r\"H/O\\s+([^,;\\.\\n]+)\", x, flags=re.IGNORECASE) if not pd.isna(x) else []\n",
    ")\n",
    "\n",
    "# Also find history of/hx of patterns\n",
    "history_of_lists = df[\"notes\"].apply(\n",
    "    lambda x: re.findall(r\"(?:history of|hx of)\\s+([^,;\\.\\n]+)\", x, flags=re.IGNORECASE) if not pd.isna(x) else []\n",
    ")\n",
    "\n",
    "# Combine history lists safely\n",
    "for i in range(len(df)):\n",
    "    if isinstance(df.at[i, \"history_list\"], list) and isinstance(history_of_lists[i], list):\n",
    "        df.at[i, \"history_list\"].extend(history_of_lists[i])\n",
    "\n",
    "# Extract additional history sections\n",
    "df[\"additional_history\"] = df[\"notes\"].apply(\n",
    "    lambda x: extract_section(x, r\"Additional history:[ \\t]*\\n?([\\s\\S]*?)(?=\\n\\s*\\n|\\n[A-Z])\", re.IGNORECASE|re.DOTALL)\n",
    ")\n",
    "\n",
    "# Create the DataFrame with the extracted information\n",
    "important = df[[\n",
    "    \"patient_id\",\n",
    "    \"reason_simple\", \n",
    "    \"reason_block\",\n",
    "    \"admit_dx\",\n",
    "    \"history_list\",\n",
    "    \"pmh_section\",\n",
    "    \"indication\",\n",
    "    \"additional_history\"\n",
    "]]\n",
    "\n",
    "# Print extraction results for each record\n",
    "print(\"\\n=== DETAILED EXTRACTION RESULTS ===\\n\")\n",
    "for i, row in df.iterrows():\n",
    "    print(f\"PATIENT ID: {row['patient_id']}\")\n",
    "    print(f\"  Reason (simple): {row['reason_simple']}\")\n",
    "    print(f\"  Reason block: {row['reason_block']}\")\n",
    "    print(f\"  Admitting Dx: {row['admit_dx']}\")\n",
    "    print(f\"  History items: {row['history_list']}\")\n",
    "    print(f\"  PMH section: {row['pmh_section']}\")\n",
    "    print(f\"  Indication: {row['indication']}\")\n",
    "    print(f\"  Additional history: {row['additional_history']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Analyze extraction success rates\n",
    "print(\"\\n=== EXTRACTION SUCCESS RATES ===\\n\")\n",
    "for col in important.columns:\n",
    "    success_rate = important[col].notna().mean() * 100\n",
    "    print(f\"{col}: {success_rate:.1f}% success\")\n",
    "\n",
    "# Count empty history lists (those will show as not null even if empty)\n",
    "empty_history_lists = sum(1 for x in important[\"history_list\"] if isinstance(x, list) and len(x) == 0)\n",
    "if empty_history_lists > 0:\n",
    "    print(f\"Note: {empty_history_lists} records have empty history_list arrays\")\n",
    "\n",
    "# Save the final extracted information to a CSV file\n",
    "important.to_csv(\"extracted_medical_notes.csv\", index=False)\n",
    "\n",
    "print(\"\\nExtraction completed and results saved to 'extracted_medical_notes.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously extracted file\n",
    "df = pd.read_csv(\"extracted_medical_notes.csv\")\n",
    "\n",
    "# Safely convert stringified lists (if history_list is stored as a string)\n",
    "import ast\n",
    "df[\"history_list\"] = df[\"history_list\"].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    ")\n",
    "\n",
    "# Create the 'important_notes' column with formatted output\n",
    "def combine_notes(row):\n",
    "    parts = []\n",
    "\n",
    "    if pd.notna(row.get(\"reason_simple\")):\n",
    "        parts.append(f\"Reason:\\n{row['reason_simple']}\")\n",
    "    if pd.notna(row.get(\"reason_block\")):\n",
    "        parts.append(f\"Detailed Reason for Examination:\\n{row['reason_block']}\")\n",
    "    if pd.notna(row.get(\"admit_dx\")):\n",
    "        parts.append(f\"Admitting Diagnosis:\\n{row['admit_dx']}\")\n",
    "    if isinstance(row.get(\"history_list\"), list) and row[\"history_list\"]:\n",
    "        parts.append(f\"History Items:\\n{', '.join(row['history_list'])}\")\n",
    "    if pd.notna(row.get(\"pmh_section\")):\n",
    "        parts.append(f\"Past Medical History:\\n{row['pmh_section']}\")\n",
    "    if pd.notna(row.get(\"indication\")):\n",
    "        parts.append(f\"Indication:\\n{row['indication']}\")\n",
    "    if pd.notna(row.get(\"additional_history\")):\n",
    "        parts.append(f\"Additional History:\\n{row['additional_history']}\")\n",
    "\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "# Apply the function\n",
    "df[\"important_notes\"] = df.apply(combine_notes, axis=1)\n",
    "\n",
    "# Select only patient_id and important_notes\n",
    "final_df = df[[\"patient_id\", \"important_notes\"]]\n",
    "\n",
    "# Save the final result\n",
    "final_df.to_csv(\"compiled_important_notes.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Successfully created 'compiled_important_notes.csv' with formatted notes.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
