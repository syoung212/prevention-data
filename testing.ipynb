{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install --upgrade google-cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install db_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-bigquery\n",
    "\n",
    "!gcloud --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth list\n",
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import db_dtypes as db_dtypes\n",
    "\n",
    "client = bigquery.Client(project=project_name)\n",
    "\n",
    "# List datasets in the specified project\n",
    "datasets = list(client.list_datasets())\n",
    "\n",
    "# Print the dataset names\n",
    "for dataset in datasets:\n",
    "    print(dataset.dataset_id)\n",
    "\n",
    "# TODO(developer): Set table_id to the ID of the destination table.\n",
    "# table_id = \"your-project.your_dataset.your_table_name\"\n",
    "sql = f'SELECT * FROM `{project_name}.mimic3_v1_4.PATIENTS`'\n",
    "query_job = client.query(sql)\n",
    "\n",
    "# df = query_job.to_dataframe()\n",
    "rows = query_job.result()\n",
    "patients_df = pd.DataFrame([dict(row) for row in rows])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "patients_df['GENDER'].value_counts().plot(kind='bar')\n",
    "\n",
    "plt.title('Gender Distribution in MIMIC-III Patient Dataset')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitarray\n",
    "from bitarray import bitarray\n",
    "import hashlib\n",
    "import random\n",
    "import math\n",
    "random.seed(0)\n",
    "\n",
    "class BloomFilter(object):\n",
    "    def __init__(self, size, hash_count):\n",
    "        \"\"\"\n",
    "        size: size of bit array\n",
    "        hash_count: number of hash functions to use\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.hash_count = hash_count\n",
    "        self.bit_array = bitarray(size)\n",
    "        self.bit_array.setall(0)\n",
    "        self.hash_param = []\n",
    "        i=0\n",
    "        while i<hash_count:\n",
    "            a=random.randint(1,9999)\n",
    "            b=random.randint(1,9999)\n",
    "            p = self.generate_large_prime(30)\n",
    "            self.hash_param.append((a,b,p))\n",
    "            i+=1\n",
    "            \n",
    "    def generate_large_prime(self, bit_size):\n",
    "\n",
    "        random_number = random.getrandbits(bit_size)\n",
    "        \n",
    "        while not self.is_prime(random_number):\n",
    "            random_number = random.getrandbits(bit_size)\n",
    "        return random_number\n",
    "\n",
    "    def is_prime(self, number):\n",
    "        if number % 2 == 0:\n",
    "            return False\n",
    "\n",
    "\n",
    "        for i in range(3, int(math.sqrt(number)) + 1, 2):\n",
    "            if number % i == 0:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "    def calculate_hash(self,item,hash_params):\n",
    "        item_val = (hash_params[0]*item + hash_params[1])%hash_params[2]\n",
    "        #print (item_val,hash_params[2])\n",
    "        return item_val\n",
    "    \n",
    "    \n",
    "    def add(self, item):\n",
    "        \"\"\"\n",
    "        Add an item to the filter\n",
    "        \"\"\"\n",
    "        for p in self.hash_param:\n",
    "            self.bit_array[self.calculate_hash(item, p)% self.size] = 1\n",
    "        \n",
    "    def lookup(self, item):\n",
    "        \"\"\"\n",
    "        Check for existence of an item in filter\n",
    "        \"\"\"\n",
    "\n",
    "        for p in self.hash_param:\n",
    "            if self.bit_array[self.calculate_hash(item, p)% self.size] == 0:\n",
    "                return False\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to join the admissions table with the diagnoses table\n",
    "sql = f\"\"\"\n",
    "SELECT\n",
    "    a.SUBJECT_ID,\n",
    "    a.HADM_ID,\n",
    "    d.ICD9_CODE,\n",
    "    a.ADMITTIME\n",
    "FROM\n",
    "    `{project_name}.mimic3_v1_4.ADMISSIONS` as a\n",
    "LEFT JOIN\n",
    "    `{project_name}.mimic3_v1_4.DIAGNOSES_ICD` as d\n",
    "ON\n",
    "    a.SUBJECT_ID = d.SUBJECT_ID AND a.HADM_ID = d.HADM_ID\n",
    "\"\"\"\n",
    "query_job = client.query(sql)\n",
    "rows = query_job.result()\n",
    "\n",
    "diagnoses_df = pd.DataFrame([dict(row) for row in rows])\n",
    "\n",
    "diagnoses_df['DIAGNOSIS_FLAG'] = diagnoses_df['ICD9_CODE'].notnull()\n",
    "\n",
    "# Left join because want to keep all admissions to the hospital, and add false to flag if no diagnosis was given\n",
    "\n",
    "ground_truth_df = diagnoses_df[['SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'ICD9_CODE', 'DIAGNOSIS_FLAG']]\n",
    "\n",
    "ground_truth_df['DIAGNOSIS_FLAG'].fillna(False, inplace=True)\n",
    "print(\"Ground Truth DataFrame:\", len(ground_truth_df))\n",
    "print(ground_truth_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each ICD9_CODE\n",
    "icd9_counts = diagnoses_df['ICD9_CODE'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "icd9_counts.head(20).plot(kind='bar')\n",
    "\n",
    "plt.title('Top 20 Most Common ICD-9 Diagnoses')\n",
    "plt.xlabel('ICD-9 Code')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd_d_sql = f'SELECT * FROM `{project_name}.mimic3_v1_4.D_ICD_DIAGNOSES`'\n",
    "icd_df_job = client.query(icd_d_sql)\n",
    "icd_df_rows = icd_df_job.result()\n",
    "icd_df = pd.DataFrame([dict(row) for row in icd_df_rows])\n",
    "icd_df = icd_df.dropna(how='all')\n",
    "\n",
    "ground_truth_df = pd.merge(diagnoses_df, icd_df, on=['ICD9_CODE'], how='left')\n",
    "print(ground_truth_df.head(5))\n",
    "\n",
    "icd9_counts = ground_truth_df['SHORT_TITLE'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "icd9_counts.head(20).plot(kind='bar')\n",
    "\n",
    "plt.title('Top 20 Most Common ICD-9 Diagnoses')\n",
    "plt.xlabel('ICD-9 Code')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patients_df = patients_df.dropna(how='all')\n",
    "admissions_sql = f'SELECT * FROM `{project_name}.mimic3_v1_4.ADMISSIONS`'\n",
    "admissions_query_job = client.query(admissions_sql)\n",
    "admissions_rows = admissions_query_job.result()\n",
    "admissions_df = pd.DataFrame([dict(row) for row in admissions_rows])\n",
    "admissions_df = admissions_df.dropna(how='all')\n",
    "print(admissions_df['SUBJECT_ID'].isin(patients_df['SUBJECT_ID']).all())  # True means filtering is unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloom Filter Testing\n",
    "\n",
    "bloom_filter = BloomFilter(size=100000, hash_count=5)\n",
    "\n",
    "for subject_id in patients_df['SUBJECT_ID']:\n",
    "    bloom_filter.add(subject_id)\n",
    "\n",
    "filtered_admissions_df = admissions_df[admissions_df['SUBJECT_ID'].apply(\n",
    "    lambda x: bloom_filter.lookup(x))]\n",
    "\n",
    "result_df = pd.merge(patients_df, filtered_admissions_df, on='SUBJECT_ID', how='inner')\n",
    "\n",
    "print(\"Patients DataFrame length\", len(patients_df))\n",
    "print(\"Admissions DataFrame length\", len(admissions_df))\n",
    "print(\"Filtered DataFrame length\", len(filtered_admissions_df))\n",
    "print(\"Patient-Admissions DataFrame length\", len(result_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloom Filter Testing with Diagnoses Table\n",
    "diagnoses_sql = f\"SELECT * FROM `{project_name}.mimic3_v1_4.DIAGNOSES_ICD`\"\n",
    "diagnoses = client.query(diagnoses_sql)\n",
    "diagnoses_rows = diagnoses.result()\n",
    "diagnoses_icd_df = pd.DataFrame([dict(row) for row in diagnoses_rows])\n",
    "print(admissions_df['SUBJECT_ID'].isin(diagnoses_icd_df['SUBJECT_ID']).all())  # True means filtering is unnecessary\n",
    "print(patients_df['SUBJECT_ID'].isin(diagnoses_icd_df['SUBJECT_ID']).all())  # True means filtering is unnecessary\n",
    "diagnoses_icd_df = diagnoses_icd_df.dropna(how='all')\n",
    "\n",
    "print(\"Diagnoses DataFrame length\", len(diagnoses_icd_df))\n",
    "print(\"Patient-Admissions DataFrame length\", len(result_df))\n",
    "\n",
    "filtered_admissions_df = diagnoses_icd_df[diagnoses_icd_df['SUBJECT_ID'].apply(\n",
    "    lambda x: bloom_filter.lookup(x))]\n",
    "\n",
    "print(\"Filtered Admissions DataFrame length\", len(filtered_admissions_df))\n",
    "result_df = pd.merge(patients_df, diagnoses_icd_df, on='SUBJECT_ID', how='inner')\n",
    "\n",
    "print(\"Patient-Admissions-Diagnoses DataFrame length\", len(result_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeevents_sql = f\"\"\"\n",
    "SELECT SUBJECT_ID, HADM_ID, ITEMID, VALUE, VALUEUOM, FLAG  \n",
    "FROM `{project_name}.mimic3_v1_4.LABEVENTS`  \n",
    "WHERE ITEMID IN (50983, 50971, 50912, 51006, 50907, 50909, 50906, 50954, 51000, 50960, 50902, 50809, 50910, 50970, 50893, 51274, 51237, 50882)  \n",
    "AND DATE(CHARTTIME) >= DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR)\n",
    "AND RAND() <= 0.1;\n",
    "\"\"\"\n",
    "labeevents_job = client.query(labeevents_sql)\n",
    "labeevents_rows = labeevents_job.result()\n",
    "labeevents_df = pd.DataFrame([dict(row) for row in labeevents_rows])\n",
    "labeevents_df = labeevents_df.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses_sql = f'SELECT * FROM `{project_name}.mimic3_v1_4.DIAGNOSES_ICD`'\n",
    "diag_job = client.query(diagnoses_sql)\n",
    "diag_rows = diag_job.result()\n",
    "diag_df = pd.DataFrame([dict(row) for row in diag_rows])\n",
    "diag_df = diag_df.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#medical history experimentation\n",
    "medical_history_df = diagnoses_df.merge(admissions_df[['SUBJECT_ID', 'HADM_ID', 'ADMITTIME']], on=['SUBJECT_ID', 'HADM_ID'])\n",
    "medical_history_df['ADMITTIME'] = pd.to_datetime(medical_history_df['ADMITTIME'])\n",
    "\n",
    "#we want to only count the number of past diagnoses for each patient\n",
    "medical_history_df = medical_history_df.groupby('SUBJECT_ID').apply(\n",
    "    lambda group: group[group['ADMITTIME'] < group['ADMITTIME'].max()]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "past_diagnoses_count = medical_history_df.groupby('SUBJECT_ID')['ICD9_CODE'].nunique().reset_index()\n",
    "past_diagnoses_count.rename(columns={'ICD9_CODE': 'TOTAL_PAST_DIAGNOSES'}, inplace=True)\n",
    "\n",
    "# if there were no previous diagnoses, change the NA to 0\n",
    "patients_medical_df = pd.merge(result_df, past_diagnoses_count, on='SUBJECT_ID', how='left')\n",
    "patients_medical_df['TOTAL_PAST_DIAGNOSES'].fillna(0, inplace=True) \n",
    "\n",
    "# add in lab events\n",
    "feature_df = pd.merge(patients_medical_df, labeevents_df[['HADM_ID', 'ITEMID', 'VALUE']], \n",
    "                            on='HADM_ID', how='left')\n",
    "\n",
    "\n",
    "#final joined dataset for feature analysis\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key_ai = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key = api_key_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set your API key\n",
    "openai.api_key = \"Enter your OpenAI API key here\"\n",
    "\n",
    "# Simple function to create a prompt from patient data\n",
    "def get_prompt(row):\n",
    "    prompt = f\"\"\"\n",
    "    Patient information:\n",
    "    - Age: {row.get('AGE', 'Unknown')}\n",
    "    - Gender: {row.get('GENDER', 'Unknown')}\n",
    "    - Past diagnoses: {int(row['TOTAL_PAST_DIAGNOSES'])}\n",
    "    - Death Flag: {'Yes' if row.get('DOD', None) is not None else 'No'}\n",
    "    \n",
    "    Lab results:\n",
    "    \"\"\"\n",
    "    \n",
    "    # can use itemid if that is provided\n",
    "    if 'ITEMID' in row and 'VALUE' in row:\n",
    "        prompt += f\"- Lab {row['ITEMID']}: {row['VALUE']}\\n\"\n",
    "    \n",
    "    prompt += \"\\nWhat is the most likely diagnosis? Include ICD9 code.\"\n",
    "    return prompt\n",
    "\n",
    "def get_diagnosis(prompt):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a medical expert. Provide diagnoses with ICD9 codes.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Sample a few rows to test\n",
    "sample = feature_df.sample(3)\n",
    "results = []\n",
    "\n",
    "for _, row in sample.iterrows():\n",
    "    prompt = get_prompt(row)\n",
    "    \n",
    "    diagnosis = get_diagnosis(prompt)\n",
    "    results.append({\n",
    "        'HADM_ID': row.get('HADM_ID', 'Unknown'),\n",
    "        'predicted_diagnosis': diagnosis\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "print(results_df['predicted_diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = 249\n",
    "\n",
    "note_events_sql = f\"\"\"\n",
    "SELECT SUBJECT_ID, HADM_ID, DESCRIPTION, ISERROR, TEXT, CHARTDATE\n",
    "FROM (\n",
    "  SELECT *, ROW_NUMBER() OVER (PARTITION BY SUBJECT_ID ORDER BY CHARTDATE DESC) AS rn\n",
    "  FROM `{project_name}.mimic3_v1_4.NOTEEVENTS`\n",
    "  WHERE SUBJECT_ID = {patient_id}\n",
    ")\n",
    "WHERE rn = 1\n",
    "\"\"\"\n",
    "\n",
    "single_note_job = client.query(note_events_sql)\n",
    "single_note_rows = single_note_job.result()\n",
    "single_note_df = pd.DataFrame([dict(row) for row in single_note_rows])\n",
    "single_note_df = single_note_df.dropna(how='all')\n",
    "print(single_note_df.head(5))\n",
    "print(len(single_note_df))\n",
    "print(single_note_df['TEXT'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_events_sql = f\"\"\"\n",
    "SELECT SUBJECT_ID, HADM_ID, DESCRIPTION, ISERROR, TEXT, CHARTDATE\n",
    "FROM (\n",
    "  SELECT *, ROW_NUMBER() OVER (PARTITION BY SUBJECT_ID ORDER BY CHARTDATE DESC) AS rn\n",
    "  FROM `{project_name}.mimic3_v1_4.NOTEEVENTS`\n",
    "  WHERE HADM_ID IS NOT NULL\n",
    ")\n",
    "WHERE rn = 1\n",
    "\"\"\"\n",
    "\n",
    "note_job = client.query(note_events_sql)\n",
    "note_rows = note_job.result()\n",
    "note_df = pd.DataFrame([dict(row) for row in note_rows])\n",
    "note_df = note_df.dropna(how='all')\n",
    "print(note_df.head(5))\n",
    "print(len(note_df))\n",
    "print(note_df['TEXT'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "filename = 'patient_notes.csv'\n",
    "header = ['patient_id', 'notes']\n",
    "rows = note_df[['SUBJECT_ID', 'TEXT']].values.tolist()\n",
    "\n",
    "with open(filename, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    \n",
    "    csvwriter.writerow(header)\n",
    "    csvwriter.writerows(rows)\n",
    "\n",
    "print(f\"Created CSV file: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of entries in CSV file\", len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking that all subject ids from notes_df are also in the ground truth\n",
    "\n",
    "ids_ground_truth = set(test_ground_truth['SUBJECT_ID'])\n",
    "ids_notes = set(note_df['SUBJECT_ID'])\n",
    "\n",
    "missing_subject_ids = ids_ground_truth - ids_notes\n",
    "\n",
    "print(\"Missing Patients\", missing_subject_ids, len(missing_subject_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created CSV file: ground_truth.csv\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "\n",
    "filename = 'ground_truth.csv'\n",
    "header = ['patient_id', 'diagnosis']\n",
    "rows = test_ground_truth[['SUBJECT_ID', 'SHORT_TITLE']].values.tolist()\n",
    "\n",
    "with open(filename, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    \n",
    "    csvwriter.writerow(header)\n",
    "    csvwriter.writerows(rows)\n",
    "\n",
    "print(f\"Created CSV file: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created CSV file: primary_diagnosis.csv\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "\n",
    "filename = 'primary_diagnosis.csv'\n",
    "header = ['patient_id', 'diagnosis']\n",
    "rows = one_diagnosis[['SUBJECT_ID', 'PRIMARY_DIAGNOSIS']].values.tolist()\n",
    "\n",
    "with open(filename, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    \n",
    "    csvwriter.writerow(header)\n",
    "    csvwriter.writerows(rows)\n",
    "\n",
    "print(f\"Created CSV file: {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
